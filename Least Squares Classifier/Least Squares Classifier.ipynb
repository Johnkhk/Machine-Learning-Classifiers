{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE174 MiniProject 1\n",
    "## Problem 1: Least Squares Classifier\n",
    "## Kwok Hung Ho A15151703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import numpy as geek  \n",
    "X = loadmat(\"mnist.mat\")\n",
    "data_train = X['trainX'] # Training set digits \n",
    "data_label = X['trainY'] # Training set labels\n",
    "data_test = X['testX'] # Training set digits \n",
    "data_test_label = X['testY']\n",
    "inv   = np.linalg.inv\n",
    "pinv  = np.linalg.pinv\n",
    "solve = np.linalg.solve\n",
    "rank  = np.linalg.matrix_rank\n",
    "norm  = np.linalg.norm\n",
    "det   = np.linalg.det\n",
    "solve = np.linalg.solve\n",
    "lstsq = np.linalg.lstsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the functions used are declared below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onevsall(data, label):\n",
    "    H = []\n",
    "    G = []\n",
    "    theta = np.array([])\n",
    "    aggG =  np.array([])\n",
    "    for t in range(10):\n",
    "        G=[]\n",
    "        H=[]\n",
    "        for i in range(len(data)):\n",
    "            if (label[0,i] == t):\n",
    "                H.append(data[i,:])\n",
    "                G.append(1)\n",
    "            else:\n",
    "                H.append(data[i,:])\n",
    "                G.append(-1)\n",
    "        Gnp = np.array(G)\n",
    "        aggG = np.append(aggG, Gnp)\n",
    "        beta = lstsq(data, Gnp, rcond=None)[0]\n",
    "        theta = np.append(theta,beta)\n",
    "    theta = theta.reshape(10, 784)\n",
    "    aggG = aggG.reshape(len(data),10)\n",
    "    return theta, aggG\n",
    "\n",
    "def onevsallconfusionmatrix(data, label, theta):\n",
    "    confusion = np.zeros(100).reshape(10,10)\n",
    "    #theta, aggG = onevsall(data, label)\n",
    "    raw_label = data@(np.transpose(theta))\n",
    "    a=[]\n",
    "    predictlabel = []\n",
    "    for i in range(len(data)):\n",
    "        a = np.argmax(raw_label[i,:])\n",
    "        b = label[:,i]\n",
    "        confusion[a,b] = confusion[a,b]+1\n",
    "        confusion = confusion.astype(int) \n",
    "        predictlabel.append(a)\n",
    "    print(confusion)\n",
    "    return confusion, predictlabel\n",
    "    \n",
    "def onevsone(data, label):\n",
    "    #TO TEST USE H[i]@theta[i]=aggG[i] i from 0 to 44d\n",
    "    #aggH and aggG are lists of numpy arrrays\n",
    "    theta = np.array([])\n",
    "    aggH = []\n",
    "    aggG =  []\n",
    "    #thetadict = {}\n",
    "    #labeldict = {}\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i<j:\n",
    "                H = []\n",
    "                G = []\n",
    "                for x in range(len(data)):\n",
    "                    if (data_label[0,x] == i):\n",
    "                        H.append(data[x,:])\n",
    "                        G.append(1)\n",
    "                    elif (label[0,x] == j):\n",
    "                        H.append(data[x,:])\n",
    "                        G.append(-1)\n",
    "                H = np.array(H)\n",
    "                aggH.append(H)\n",
    "                G = np.array(G)\n",
    "                aggG.append(G)\n",
    "                beta = lstsq(H, G, rcond=None)[0]\n",
    "                theta = np.append(theta,beta)\n",
    "                #theta = np.transpose(theta)\n",
    "                #print(aggG.shape)\n",
    "                #print(Gnp.shape)\n",
    "                #print(theta.shape)\n",
    "                #print(H.shape)\n",
    "    theta = theta.reshape(45,784)\n",
    "    theta = theta.T\n",
    "    return theta, aggG, aggH\n",
    "\n",
    "def onevsone45mapping():\n",
    "    main_array = np.empty((0,9), int)\n",
    "    init = 9\n",
    "    start_val = 9\n",
    "    for i in range(9):\n",
    "        vals = 9 - i\n",
    "        #for j in range(vals):\n",
    "        zeroes = np.zeros(i)\n",
    "        if i == 0:\n",
    "            zeroes = np.zeros(0)\n",
    "            values = np.arange(0, 9)\n",
    "            init = init - 1\n",
    "        else:\n",
    "            values = np.arange(start_val, start_val + vals)\n",
    "            start_val = start_val + init\n",
    "            init = init - 1\n",
    "        append_array = np.hstack([zeroes, values])\n",
    "        #print(append_array)\n",
    "        main_array = np.vstack([main_array, append_array])\n",
    "        #print(main_array)\n",
    "    return main_array\n",
    "\n",
    "def onevsoneconfusionmatrix(data, label, theta): #theta should be obtained from data_train\n",
    "    thetaij = onevsone45mapping() # maps 0-1, 0-2,...8-9, t0 0,, 1,...44\n",
    "    A = geek.sign(data@theta) \n",
    "    votetable = np.zeros((len(data[:,0]),10))\n",
    "    predlabel=[]\n",
    "    for i in range(len(data[:,0])): # making the vote table\n",
    "        for j in range(45):\n",
    "            if A[i,j] ==1:\n",
    "                arg = np.argwhere(thetaij==j)[0][0]\n",
    "                votetable[i,arg] +=1\n",
    "            elif A[i,j] ==-1:\n",
    "                arg = np.argwhere(thetaij==j)[0][1]\n",
    "                votetable[i,arg+1] +=1    \n",
    "        predlabel.append(np.argmax(votetable[i]))\n",
    "        \n",
    "    confusion = np.zeros((10,10)) # Making the confusion matrix\n",
    "    for i in range(len(data[:,0])):\n",
    "        a = predlabel[i]\n",
    "        b = label[:,i]\n",
    "        confusion[a,b] = confusion[a,b]+1\n",
    "        confusion = confusion.astype(int)\n",
    "    #print(confusion)\n",
    "    return predlabel, thetaij, confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Vs One\n",
    "Obtained weights via least squares with training data and tested it on the test data. Results are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      " [ 0.  9. 10. 11. 12. 13. 14. 15. 16.]\n",
      " [ 0.  0. 17. 18. 19. 20. 21. 22. 23.]\n",
      " [ 0.  0.  0. 24. 25. 26. 27. 28. 29.]\n",
      " [ 0.  0.  0.  0. 30. 31. 32. 33. 34.]\n",
      " [ 0.  0.  0.  0.  0. 35. 36. 37. 38.]\n",
      " [ 0.  0.  0.  0.  0.  0. 39. 40. 41.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. 42. 43.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. 44.]]\n",
      "[[ 959    0    9    8    2    8    8    1    8    6]\n",
      " [   0 1122   20    2    3    7    5   18   16    5]\n",
      " [   2    3  931   14    6    3    8   16    7    1]\n",
      " [   1    3   12  930    1   31    0    3   20   12]\n",
      " [   0    0   11    1  926    8    5   11   11   29]\n",
      " [   4    1    3   21    1  788   15    1   38   10]\n",
      " [   8    3   12    2    7   18  915    0    9    0]\n",
      " [   3    1    8    8    4    2    0  954   10   22]\n",
      " [   2    2   26   20    4   22    2    2  845    5]\n",
      " [   1    0    0    4   28    5    0   22   10  919]]\n"
     ]
    }
   ],
   "source": [
    "theta, aggG, aggH = onevsone(data_train, data_label)\n",
    "predlabel, thetaij, confusion = onevsoneconfusionmatrix(data_test, data_test_label, theta)\n",
    "print(thetaij)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first matrix is the mapping matrix for the one vs one classifier. Using this matrix, I can correctly increment the votetable for each picture, thus showing how many votes are cast for each class. After that, I obtained the predicted labels through the argument in the votetable with the highest votes. The confusion matrix is then obtained by comparing the predicted label and the actual label.\n",
    "\n",
    "Below is the confusion matrix obtained from running it on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5811    2   52   27   14   46   27   11   33   22]\n",
      " [   2 6633   64   41   20   45   19   65  194   17]\n",
      " [  14   28 5518  110   19   35   29   56   49   16]\n",
      " [   8   22   62 5619    3  160    1    6  107   84]\n",
      " [  11    6   54    9 5570   26   27   71   44  144]\n",
      " [  21   10   20  122    9 4909   79    9  155   27]\n",
      " [  22    2   45   22   21  106 5704    1   36    3]\n",
      " [   4    8   36   48   17    7    0 5874   26  130]\n",
      " [  29   23   92   91    6   67   31    8 5147   37]\n",
      " [   1    8   15   42  163   20    1  164   60 5469]]\n",
      "number of errors: 3746\n",
      "error rate is: 0.062433333333333334\n"
     ]
    }
   ],
   "source": [
    "theta, aggG, aggH = onevsone(data_train, data_label)\n",
    "predlabel, thetaij, confusion = onevsoneconfusionmatrix(data_train, data_label, theta)\n",
    "print(confusion)\n",
    "finderror(predlabel, data_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the error:\n",
    "This error is for theta found from the training set applied to the test set. (where theta is the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors: 711\n",
      "error rate is: 0.0711\n"
     ]
    }
   ],
   "source": [
    "predlabel, thetaij, confusion = onevsoneconfusionmatrix(data_test, data_test_label, theta) \n",
    "finderror(predlabel, data_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error seen above is about 0.0711, corresponding to 7.1%. The error represents the results of the weights obtained from training data on the test data. \n",
    "\n",
    "# One Vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5665    1   93   42    9  144  107   53   82   65]\n",
      " [   8 6514  258  142   97   70   66  187  527   60]\n",
      " [  19   36 4802  183   56   30   65   43   58   25]\n",
      " [  18   16  149 5206    8  520    1   57  223  113]\n",
      " [  26   10  101   29 5125   82   59  153  123  355]\n",
      " [  43   29   11   92   49 3786   81    9  234    8]\n",
      " [  73   15  237   58   48  196 5491    3   50    4]\n",
      " [   3   13   84  109   23   38    0 5391   21  488]\n",
      " [  62  103  203  139   83  404   46   18 4358   53]\n",
      " [   6    5   20  131  344  151    2  351  175 4778]]\n",
      "number of errors: 8884\n",
      "error rate is: 0.14806666666666668\n"
     ]
    }
   ],
   "source": [
    "theta, aggG = onevsall(data_train, data_label)\n",
    "confusion, predictlabel = onevsallconfusionmatrix(data_train, data_label, theta)\n",
    "finderror(predictlabel, data_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The confusion matrix above is for the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 942    0   17    4    0   20   17    5   17   18]\n",
      " [   0 1107   56   15   23   17    9   38   54   10]\n",
      " [   2    2  809   26    6    2   10   18    9    2]\n",
      " [   2    2   28  887    3   84    0    8   32   15]\n",
      " [   1    1   16    2  872   19   21   20   27   72]\n",
      " [   7    1    0   14    5  624   20    0   42    1]\n",
      " [  15    5   42    9   10   22  872    1   15    1]\n",
      " [   2    2   21   21    2   13    0  877   12   76]\n",
      " [   7   15   39   21   13   69    9    3  743   13]\n",
      " [   2    0    4   11   48   22    0   58   23  801]]\n",
      "number of errors: 1466\n",
      "error rate is: 0.1466\n"
     ]
    }
   ],
   "source": [
    "theta, aggG = onevsall(data_train, data_label)\n",
    "confusion, predictlabel = onevsallconfusionmatrix(data_test, data_test_label, theta)\n",
    "finderror(predictlabel, data_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The confusion matrix above is for the test data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation:\n",
    "The error for the one-vs-all classifier has about 14.5% error rate while the one-vs-one classifier has about a 7.1% error rate for the test data. The one-vs-one classifier seems to out perform the one-vs-all classifier by twice the ammount. \n",
    "\n",
    "Considering the test data:\n",
    "For the one-vs-one classifier, the number 1 seems to be the easiest to recognizes with 1122 correct guesses, while the most difficult to recognize is the number 5 with 788 correct guesses. \n",
    "\n",
    "For the one-vs-all classifier, the easiset to recognize seems to be the number 1 as well with 1107 correct guesses, and the most difficult to guess is also the number 5 with 624 correct guesses. \n",
    "\n",
    "It seems that numbers that look similar such as 5 and 8 have a lower amount of correct guesses, while numbers such as 1 or 0 have the best guesses. The one-vs-one classifier might outperform the one-vs-all classifier because it directly compares 2 numbers while the one-vs-all compares it to a bundle of numbers. The binary classification as such is more certain when it is one-to-one, since the appearance of multiple similar numbers may less affect the direct comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
